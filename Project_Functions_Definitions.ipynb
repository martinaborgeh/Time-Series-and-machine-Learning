{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708539dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from scipy.sparse.linalg import lsmr\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from mlxtend.regressor import StackingRegressor,StackingCVRegressor\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pymannkendall as mk\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "\n",
    "\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0dd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b5c07b",
   "metadata": {},
   "source": [
    "# Time Series Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4637b0",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647d35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_GNSS_Data(gnss_file_path):\n",
    "    data = pd.read_csv(gnss_file_path)\n",
    "    data['date'] = pd.to_datetime(data[['YEAR', 'MONTH', 'DAY']])\n",
    "    data.set_index('date', inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab6bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_ERA5_Data(ERA5_file_path):\n",
    "    ERA_5 = pd.read_csv(ERA5_file_path)\n",
    "    ERA_5['date'] = pd.to_datetime(ERA_5['date'])\n",
    "    ERA_5.set_index('date', inplace=True)\n",
    "    return ERA_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be06a5",
   "metadata": {},
   "source": [
    "## Station Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584d4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Station_Extraction(STATION_NAME,data):\n",
    "    return data[data['STN']== STATION_NAME]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5415a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERA5_STATION_EXTRACTION(STATION_NAME,ERA_5):\n",
    "    return ERA_5[ERA_5['station']== STATION_NAME]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba937a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_individual_Year_of_Station(EXTRACTED_STATION,YEAR):\n",
    "    return EXTRACTED_STATION[EXTRACTED_STATION['YEAR'] == YEAR]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e020281",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed1b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corelation_Analysis(Station,data,ERA_5):\n",
    "    ERA_STATION = ERA5_STATION_EXTRACTION(Station,ERA_5)\n",
    "    GNSS_STATION = Station_Extraction(Station,data)\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.plot(ERA_STATION['ztd [m]'])\n",
    "    plt.plot(GNSS_STATION['ZTD_igs'])\n",
    "    plt.show()\n",
    "    return ERA_STATION['ztd [m]'].corr(GNSS_STATION['ZTD_igs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92419fad",
   "metadata": {},
   "source": [
    "## Stationarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789251a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stationarity(STATION_NAME):\n",
    "    result = adfuller(STATION_NAME['ZTD_igs'])\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e4fba",
   "metadata": {},
   "source": [
    "## Mankendall Trend Test Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mankendall_Trend_Test(STATION_NAME):\n",
    "    result = mk.hamed_rao_modification_test(STATION_NAME['ZTD_igs'])\n",
    "    print('Trend:', result.trend)\n",
    "    print('H:', result.h)\n",
    "    print('p-value:', result.p)\n",
    "    print('Z:', result.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45dc6f",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e922ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739aa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinglePlot(EXTRACTED_STATION_NAME):\n",
    "    \n",
    "    fig, axs = plt.subplots(5, 2,figsize=(15, 14))\n",
    "    annual_decomposition = sm.tsa.seasonal_decompose(EXTRACTED_STATION_NAME['ZTD_igs'], model='additive',period=30)\n",
    "    semi_annual_decomposition = sm.tsa.seasonal_decompose(EXTRACTED_STATION_NAME['ZTD_igs'], model='additive',period=183)\n",
    "    inter_annual_decomposition = sm.tsa.seasonal_decompose(EXTRACTED_STATION_NAME['ZTD_igs'], model='additive',period=365)\n",
    "    \n",
    "    df = EXTRACTED_STATION_NAME\n",
    "    df['Inter-Annual'] = df['ZTD_igs'].diff(12)  # Assuming monthly data with a frequency of 12\n",
    "    axs[0, 0].plot(EXTRACTED_STATION_NAME['ZTD_igs'].index, EXTRACTED_STATION_NAME['ZTD_igs'],color='red',label = 'ztd')\n",
    "    axs[0, 0].set_title('ztd')\n",
    "\n",
    "    axs[0, 1].plot(df.index, df['Inter-Annual'],color='orange',label = 'ztd')\n",
    "    axs[0, 1].set_title('Inter Annual Variability')\n",
    "    \n",
    "    axs[1, 1].plot(annual_decomposition.trend.index, annual_decomposition.trend,color='blue',label = 'ztd')\n",
    "    axs[1, 1].set_title('Annual Trend')\n",
    "    axs[1, 0].plot(annual_decomposition.seasonal.index, annual_decomposition.seasonal,color='orange',label = 'ztd')\n",
    "    axs[1, 0].set_title('Annual Seasonal')\n",
    "    axs[2, 1].plot(semi_annual_decomposition.trend.index, semi_annual_decomposition.trend,color='blue',label = 'ztd')\n",
    "    axs[2, 1].set_title('Semi Annual Trend')\n",
    "    axs[2, 0].plot(semi_annual_decomposition.seasonal.index, semi_annual_decomposition.seasonal,color='orange',label = 'ztd')\n",
    "    axs[2, 0].set_title('Semi Annual Seasonal')\n",
    "   \n",
    "    \n",
    "    axs[3, 1].plot(inter_annual_decomposition.trend.index, inter_annual_decomposition.trend,color='blue',label = 'ztd')\n",
    "    axs[3, 1].set_title('Inter Annual Trend')\n",
    "    axs[3, 0].plot(inter_annual_decomposition.seasonal.index, inter_annual_decomposition.seasonal,color='orange',label = 'ztd')\n",
    "    axs[3, 0].set_title('Inter Annual Seasonal')\n",
    "    \n",
    "    \n",
    "    plot_pacf(EXTRACTED_STATION_NAME['ZTD_igs'],ax=axs[4, 0],color='red')\n",
    "    axs[4, 0].set_title('Partial AutoCorrelation Analysis')\n",
    "    axs[4, 0].set_ylabel('Partial Autocorrelation')\n",
    "    axs[4, 0].set_xlabel('Lags')\n",
    "    \n",
    "    plot_acf(EXTRACTED_STATION_NAME['ZTD_igs'],ax=axs[4, 1],color='red')\n",
    "    axs[4, 1].set_title('AutoCorrelation Analysis')\n",
    "    axs[4, 1].set_ylabel('Autocorrelation')\n",
    "    axs[4, 1].set_xlabel('Lags')\n",
    "  \n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c5052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Subplotting_Individual_Years(Year_STATIONS,station):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 4,figsize=(15, 8), sharey=True)\n",
    "# Plot data on subplots\n",
    "    axs[0, 0].plot(Year_STATIONS[0].index, Year_STATIONS[0]['ZTD_igs'],color = 'red')\n",
    "    axs[0, 0].set_title('2015')\n",
    "    \n",
    "    plt.setp(axs[0, 0].get_xticklabels(), rotation=45)\n",
    "\n",
    "    axs[0, 1].plot(Year_STATIONS[1].index, Year_STATIONS[1]['ZTD_igs'],color = 'red')\n",
    "    axs[0, 1].set_title('2016')\n",
    "    plt.setp(axs[0, 1].get_xticklabels(), rotation=45)\n",
    "\n",
    "    axs[1, 0].plot(Year_STATIONS[2].index, Year_STATIONS[2]['ZTD_igs'],color = 'red')\n",
    "    axs[1, 0].set_title('2017')\n",
    "    plt.setp(axs[1, 0].get_xticklabels(), rotation=45)\n",
    "\n",
    "    axs[1, 2].plot(Year_STATIONS[3].index, Year_STATIONS[3]['ZTD_igs'],color = 'red')\n",
    "    axs[1, 2].set_title('2018')\n",
    "    plt.setp(axs[1, 2].get_xticklabels(), rotation=45)\n",
    "\n",
    "    axs[1, 3].plot(Year_STATIONS[4].index, Year_STATIONS[4]['ZTD_igs'],color = 'red')\n",
    "    axs[1, 3].set_title('2019')\n",
    "    plt.setp(axs[1, 3].get_xticklabels(), rotation=45)\n",
    "\n",
    "    axs[1, 1].plot(Year_STATIONS[5].index, Year_STATIONS[5]['ZTD_igs'],color = 'red')\n",
    "    axs[1, 1].set_title('2020')\n",
    "    plt.setp(axs[1, 1].get_xticklabels(), rotation=45)\n",
    "\n",
    "    axs[0, 2].plot(Year_STATIONS[6].index, Year_STATIONS[6]['ZTD_igs'],color = 'red')\n",
    "    axs[0, 2].set_title('2021')\n",
    "    plt.setp(axs[0, 2].get_xticklabels(), rotation=45)\n",
    "\n",
    "    axs[0, 3].plot(Year_STATIONS[7].index, Year_STATIONS[7]['ZTD_igs'],color = 'red')\n",
    "    axs[0, 3].set_title('2022')\n",
    "    plt.setp(axs[0, 3].get_xticklabels(), rotation=45)\n",
    "    fig.suptitle(f'STATION {station}')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b494c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Superimpose_Different_Years_Single_Plot(Year_STATIONS):\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    plt.plot(Year_STATIONS[0].index,Year_STATIONS[0]['ZTD_igs'],color='red' )\n",
    "    plt.plot(Year_STATIONS[1].index,Year_STATIONS[1]['ZTD_igs'],color='blue' )\n",
    "    plt.plot(Year_STATIONS[2].index,Year_STATIONS[2]['ZTD_igs'],color='yellow' )\n",
    "    plt.plot(Year_STATIONS[3].index,Year_STATIONS[3]['ZTD_igs'],color='purple' )\n",
    "    plt.plot(Year_STATIONS[4].index,Year_STATIONS[4]['ZTD_igs'],color='pink' )\n",
    "    plt.plot(Year_STATIONS[5].index,Year_STATIONS[5]['ZTD_igs'],color='green' )\n",
    "    plt.plot(Year_STATIONS[6].index,Year_STATIONS[6]['ZTD_igs'],color='black' )\n",
    "    plt.plot(Year_STATIONS[7].index,Year_STATIONS[7]['ZTD_igs'],color='orange' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd2f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seasonal_TrendpPlotting(STATION_NAME):\n",
    "    df_YEARLY = STATION_NAME.resample('m').sum()\n",
    "    decomposition = sm.tsa.seasonal_decompose(df_YEARLY['ZTD_igs'], model='additive')\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    decomposition.trend.plot(ax=ax1)\n",
    "    ax1.set_title('Trend')\n",
    "    decomposition.seasonal.plot(ax=ax2)\n",
    "    ax2.set_title('Seasonal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "900b3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Atmostpheric_Plot_With_GNSS(STATION,data,ERA_5):\n",
    "    GNSS_STATION = Station_Extraction(STATION,data)\n",
    "    ERAS_STATTION = ERA5_STATION_EXTRACTION(STATION,ERA_5)\n",
    "    fig, axs = plt.subplots(2, 4,figsize=(20, 6))\n",
    "    \n",
    "\n",
    "    axs[0, 0].plot(GNSS_STATION['ZTD_igs'],color='red')\n",
    "    axs[0, 0].set_title('ZTD_igs')\n",
    "    \n",
    "    plt.setp(axs[0, 0].get_xticklabels(), rotation=45)\n",
    "#     plt.setp(ax2.get_xticklabels(), rotation=45)\n",
    "    \n",
    "\n",
    "    axs[0, 1].plot(ERAS_STATTION['T [K]'].index,ERAS_STATTION['T [K]'],color='blue')\n",
    "    axs[0, 1].set_title('Atmospheric Temperature')\n",
    "    plt.setp(axs[0, 1].get_xticklabels(), rotation=45)\n",
    "   \n",
    "    \n",
    "    axs[0, 2].plot(ERAS_STATTION['p [hPa]'].index,ERAS_STATTION['p [hPa]'],color='orange')\n",
    "    axs[0, 2].set_title('Atmospheric Pressure')\n",
    "    plt.setp(axs[0, 2].get_xticklabels(), rotation=45)\n",
    "   \n",
    "    \n",
    "    axs[0, 3].plot(ERAS_STATTION['p [hPa]'].index,ERAS_STATTION['e [hPa]'],color='black')\n",
    "    axs[0, 3].set_title('Atmospheric Partial Water Vapour Pressure')\n",
    "    plt.setp(axs[0, 3].get_xticklabels(), rotation=45)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded696a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mankendall_Trend_Test_Plot(STATION_NAME,data):\n",
    "    p_values = []\n",
    "    station_names = data ['STN'].drop_duplicates()\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    for item in STATION_NAME:\n",
    "        result = mk.hamed_rao_modification_test(item['ZTD_igs'])\n",
    "        p_values.append (result.p)\n",
    "    plt.bar(station_names.values, p_values,width=0.3)\n",
    "    plt.xticks(rotation=45)  # Rotate the x-labels by 45 degrees\n",
    "   \n",
    "#     y_value = 10\n",
    "#     plt.axhline(y=y_value, color='red', linestyle='--')\n",
    "    plt.xlabel('Station Names')\n",
    "    plt.ylabel('Mankendall P-values')\n",
    "    plt.title('Trend Analysis with Mankendall Statistics Test')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a73bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mankendall_Trend_Test_Cluster(STATION_NAME,data):\n",
    "    cluster_values = []\n",
    "    station_names = data ['STN'].drop_duplicates()\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    for item in STATION_NAME:\n",
    "        result = mk.hamed_rao_modification_test(item['ZTD_igs'])\n",
    "        trend =result.trend\n",
    "        if trend=='increasing':\n",
    "            cluster_values.append(1)\n",
    "        elif trend=='decreasing':\n",
    "            cluster_values.append(-1)\n",
    "        elif trend=='no trend':\n",
    "            cluster_values.append(0)\n",
    "    legend_mapping = { -1: \"Decreasing Trend\", 0: \"No Trend\", 1: \"Increasing Trend\"}\n",
    "    renamed_labels = [legend_mapping[label] for label in cluster_values]\n",
    "    dataframe = pd.DataFrame({'cluster_values':cluster_values,'station_names':station_names})\n",
    "    sns.scatterplot(data=dataframe, x='station_names', y='cluster_values',hue=renamed_labels, palette='tab10')\n",
    "    plt.xticks(rotation=45) \n",
    "#     plt.bar(station_names.values, p_values,width=0.3)\n",
    "#     plt.xticks(rotation=45)  # Rotate the x-labels by 45 degrees\n",
    "   \n",
    "# #     y_value = 10\n",
    "# #     plt.axhline(y=y_value, color='red', linestyle='--')\n",
    "    plt.xlabel('Station Names')\n",
    "    plt.ylabel('cluster Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c28d4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trend_and_Stationarity_Visualisation(Stations,data):\n",
    "    Mankendall_p_values = []\n",
    "    stationarity_p_values = []\n",
    "    fig, ax = plt.subplots(figsize=(13, 6))\n",
    "    station_names = data ['STN'].drop_duplicates()\n",
    "    for item in Stations:\n",
    "        \n",
    "        Mankendall_result = mk.hamed_rao_modification_test(item['ZTD_igs'])\n",
    "        Mankendall_p_values.append ( Mankendall_result.p)\n",
    "        \n",
    "        Stationarity_result = adfuller(item['ZTD_igs'])\n",
    "        stationarity_p_values.append(Stationarity_result[1])\n",
    "        \n",
    "\n",
    "    x = np.arange(len(station_names))\n",
    "    \n",
    "        \n",
    "    ax.bar(x-0.3, Mankendall_p_values,width=0.3,color = 'red',label = 'Mankendal')\n",
    "    ax.bar(x,stationarity_p_values,width=0.3,color ='blue',label = 'Stationarity')\n",
    "    \n",
    "\n",
    "    group_x = x - 0.3 / 2 \n",
    "    ax.set_xticks(group_x)\n",
    "    ax.set_xticklabels(station_names,rotation=45)\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    ax.set_ylabel('P values')\n",
    "\n",
    "\n",
    "    ax.set_title('Stationarity and Trend Visualisation')\n",
    "\n",
    " \n",
    "    ax.legend()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc29ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corelation_Analysis_Plot(Station,data,ERA_5):\n",
    "    correlation = []\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    station_names = data ['STN'].drop_duplicates()\n",
    "    for item in Station:\n",
    "        ERA_STATION = ERA5_STATION_EXTRACTION(item,ERA_5)\n",
    "        GNSS_STATION = Station_Extraction(item,data)\n",
    "        correlation.append(ERA_STATION['ztd [m]'].corr(GNSS_STATION['ZTD_igs']))\n",
    "    plt.bar(station_names, correlation,width=0.3)\n",
    "    plt.xticks(rotation=45)  \n",
    "    plt.xlabel('Station Names')\n",
    "    plt.ylabel('Correlation Values')\n",
    "    plt.title('Correlation Analysis with Adfuller Statistics Test')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eb2d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GNSS_AND_ATMOSPHERIC_PARAMETER_CORRELATTION(Station,data,ERA_5):\n",
    "    GNSS_Temperature_correlation = []\n",
    "    GNSS_Pressure_correlation = []\n",
    "    GNSS_Partial_Water_Vapour_Pressure_correlation = []\n",
    "    fig, ax = plt.subplots(figsize=(13, 6))\n",
    "    station_names = data ['STN'].drop_duplicates()\n",
    "    for item in Station:\n",
    "        ERA_STATION = ERA5_STATION_EXTRACTION(item,ERA_5)\n",
    "        GNSS_STATION = Station_Extraction(item,data)\n",
    "        GNSS_Temperature_correlation.append(ERA_STATION['T [K]'].corr(GNSS_STATION['ZTD_igs']))\n",
    "        GNSS_Pressure_correlation.append(ERA_STATION['p [hPa]'].corr(GNSS_STATION['ZTD_igs']))\n",
    "        GNSS_Partial_Water_Vapour_Pressure_correlation.append(ERA_STATION['e [hPa]'].corr(GNSS_STATION['ZTD_igs']))\n",
    "    x = np.arange(len(station_names))\n",
    "    \n",
    "    ax.bar(x-0.25, GNSS_Temperature_correlation,width=0.25,color = 'red',label = 'GNSS AND TEMPERATURE CORELLATION')\n",
    "    ax.bar(x,GNSS_Pressure_correlation,width=0.25,color ='blue',label = 'GNSSS AND PRESSURE CORRELATION')\n",
    "    ax.bar(x+0.2,GNSS_Partial_Water_Vapour_Pressure_correlation,width=0.25,color ='green',label = 'GNSS AND PARTIAL WATER VAPOUR CORRELATION')\n",
    "    \n",
    "\n",
    "    group_x = x - 0.25 / 2 \n",
    "    ax.set_xticks(group_x)\n",
    "    ax.set_xticklabels(station_names,rotation=45)\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    ax.set_ylabel('Correlation  values')\n",
    "\n",
    "\n",
    "#     ax.set_title('CORRELATION BETWEEN GNSS(ZTD_igs), TEMPERATURE(T [K]), ATMOSPHERIC PRESSURE(p [hPa]) AND ATMOSPHERIC PARTIAL WATER VAPOUR PRESSURE(e [hPa])')\n",
    "\n",
    " \n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca39d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a05cbeb4",
   "metadata": {},
   "source": [
    "# Time Series Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a82e78",
   "metadata": {},
   "source": [
    "## Autogressive Forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d75677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Autoregressive_Forecasting(Station_Name,Lag):\n",
    "    train_data = Station_Name.iloc[:-500]  # n is the number of data points to use for testing\n",
    "    test_data = Station_Name.iloc[-500:]\n",
    "\n",
    "    \n",
    "    model = sm.tsa.AutoReg(train_data['ZTD_igs'], Lag)\n",
    "  \n",
    "    ar_model = model.fit()  # maxlag determines the lag order\n",
    "\n",
    "    \n",
    "    forecast = ar_model.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "    rmse = np.sqrt(mean_squared_error(test_data['ZTD_igs'].values.flatten(), forecast.values.flatten()))\n",
    "    compare = pd.DataFrame({'Date':test_data['ZTD_igs'].index,'Actual':test_data['ZTD_igs'],'Predicted':forecast})\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(test_data['ZTD_igs'],color ='yellow',label ='tested data')\n",
    "    plt.plot(Station_Name['ZTD_igs'],color='blue',label = 'original data')\n",
    "    plt.plot(forecast,color='red',label ='prediction')\n",
    "#     ar_model.plot_predict(start=1, end=len(train_data['ZTD_igs'])-1)\n",
    "    \n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    return ar_model.summary(),compare.head(10),f'the root mean square is {rmse}',forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef089580",
   "metadata": {},
   "source": [
    "## Moving Average Forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63927a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Moving_Average(Station_Name, lag):\n",
    "    train_data = Station_Name.iloc[:-500]  # n is the number of data points to use for testing\n",
    "    test_data = Station_Name.iloc[-500:]\n",
    "    model = ARIMA(train_data['ZTD_igs'], order=(0,0,lag))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "    compare = pd.DataFrame({'Actual':test_data['ZTD_igs'],'Predicted':predictions })\n",
    "    rmse = np.sqrt(mean_squared_error(test_data['ZTD_igs'].values.flatten(), predictions.values.flatten()))\n",
    "   \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(Station_Name['ZTD_igs'], label='Original Series')\n",
    "    plt.plot(test_data['ZTD_igs'], label='tested Series',color='y')\n",
    "    plt.plot(predictions, label='MA Predictions',color='r')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    return model_fit.summary(),compare.head(10),f'the root mean square is {rmse}',predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf60ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e3c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef6633a",
   "metadata": {},
   "source": [
    "## ARIMA and SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714f6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd063cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arima_Forecasting(Station_Name,order):\n",
    "    train_data = Station_Name.iloc[:-500]  # n is the number of data points to use for testing\n",
    "    test_data = Station_Name.iloc[-500:]\n",
    "\n",
    "    arima_model = ARIMA(train_data['ZTD_igs'], order=order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_model_fit.summary()\n",
    "    arima_predictions = arima_model_fit.predict(start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "    rmse = np.sqrt(mean_squared_error(test_data['ZTD_igs'].values.flatten(), arima_predictions.values.flatten()))\n",
    "    compare = pd.DataFrame({'Actual':test_data['ZTD_igs'],'Predicted':arima_predictions})\n",
    "    plt.figure(figsize=(13,8))\n",
    "    plt.plot(Station_Name['ZTD_igs'], label='Original Series')\n",
    "    plt.plot(test_data['ZTD_igs'], label='tested data',color='y')\n",
    "    plt.plot(arima_predictions, label='ARIMA Predictions',color='r')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    return arima_model_fit.summary(),compare.head(10),f'the root mean square is {rmse}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab329c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa81a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Stationarity(differenced_data):\n",
    "    result = adfuller(differenced_data)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a58d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Mankendall_Trend_Test(differenceddata):\n",
    "    result = mk.hamed_rao_modification_test(differenceddata)\n",
    "    print('Trend:', result.trend)\n",
    "    print('H:', result.h)\n",
    "    print('p-value:', result.p)\n",
    "    print('Z:', result.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "541222fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Detrending(Station_name):\n",
    "    fig, axs = plt.subplots(2, 2,figsize=(15, 14))\n",
    "    axs[0,0].plot(Station_name['ZTD_igs'], label='Original Data')\n",
    "    axs[0,0].set_xlabel('time')\n",
    "    axs[0,0].set_ylabel('ZTD_igs')\n",
    "    axs[0, 0].set_title('Original Data')\n",
    "    \n",
    "    \n",
    "    trend_differenced_data = Station_name['ZTD_igs'].diff().dropna()\n",
    "   \n",
    "   \n",
    "    \n",
    "    axs[0,1].plot(trend_differenced_data, label='differenced')\n",
    "    axs[0,1].set_xlabel('time')\n",
    "    axs[0,1].set_ylabel('Z value')\n",
    "    axs[0, 1].set_title('Differenced Data')\n",
    "    \n",
    "    plot_pacf(trend_differenced_data,ax=axs[1, 0],color='red')\n",
    "    axs[1, 0].set_title('Partial AutoCorrelation Analysis')\n",
    "    axs[1, 0].set_ylabel('Partial Autocorrelation')\n",
    "    axs[1, 0].set_xlabel('Lags')\n",
    "    \n",
    "    plot_acf(trend_differenced_data,ax=axs[1, 1],color='red')\n",
    "    axs[1, 1].set_title('AutoCorrelation Analysis')\n",
    "    axs[1, 1].set_ylabel('Autocorrelation')\n",
    "    axs[1, 1].set_xlabel('Lags')\n",
    "    print(trend_differenced_data)\n",
    "    return pd.DataFrame({'ZTD_igs':trend_differenced_data}),check_Mankendall_Trend_Test(trend_differenced_data),check_Stationarity(trend_differenced_data)\n",
    "\n",
    "# pd.Series(differenced_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02d99987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deseasonalise(Station_name):\n",
    "    fig, axs = plt.subplots(2, 2,figsize=(15, 14))\n",
    "    axs[0,0].plot(Station_name['ZTD_igs'], label='Original Data')\n",
    "    axs[0,0].set_xlabel('time')\n",
    "    axs[0,0].set_ylabel('Z value')\n",
    "    axs[0, 0].set_title('Original Data')\n",
    "    \n",
    "    seasonal_differenced_data = Station_name['ZTD_igs'].diff(periods=366).dropna()\n",
    "    \n",
    "    \n",
    "    axs[0,1].plot(seasonal_differenced_data, label='differenced')\n",
    "    axs[0,1].set_xlabel('time')\n",
    "    axs[0,1].set_ylabel('Z value')\n",
    "    axs[0, 1].set_title('Differenced Data')\n",
    "    \n",
    "    plot_pacf(seasonal_differenced_data,ax=axs[1, 0],color='red')\n",
    "    axs[1, 0].set_title('Partial AutoCorrelation Analysis')\n",
    "    axs[1, 0].set_ylabel('Partial Autocorrelation')\n",
    "    axs[1, 0].set_xlabel('Lags')\n",
    "    \n",
    "    plot_acf(seasonal_differenced_data,ax=axs[1, 1],color='red')\n",
    "    axs[1, 1].set_title('AutoCorrelation Analysis')\n",
    "    axs[1, 1].set_ylabel('Autocorrelation')\n",
    "    axs[1, 1].set_xlabel('Lags')\n",
    "    \n",
    "    return pd.DataFrame({'ZTD_igs':seasonal_differenced_data}),check_Stationarity(seasonal_differenced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbf33522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Trend_or_Seasonal(Station_Name,differencing_type):\n",
    "    if differencing_type =='S':\n",
    "        return Deseasonalise(Station_Name)\n",
    "    elif differencing_type =='T':\n",
    "        return Detrending(Station_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9d098",
   "metadata": {},
   "source": [
    "# Machine_Learning_Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58419869",
   "metadata": {},
   "source": [
    "## Least Square Support vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "067dc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSSVR(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Least Squares Support Vector Regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : float, default=2.0\n",
    "        Regularization parameter. The strength of the regularization is\n",
    "        inversely proportional to C. Must be strictly positive.\n",
    "\n",
    "    kernel : {'linear', 'rbf'}, default='linear'\n",
    "        Specifies the kernel type to be used in the algorithm.\n",
    "        It must be 'linear', 'rbf' or a callable.\n",
    "\n",
    "    gamma : float, default = None\n",
    "        Kernel coefficient for 'rbf'\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    support_: boolean np.array of shape (n_samples,), default = None\n",
    "        Array for support vector selection.\n",
    "\n",
    "    alpha_ : array-like\n",
    "        Weight matrix\n",
    "\n",
    "    bias_ : array-like\n",
    "        Bias vector\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=2.0, kernel='linear', gamma=None):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def fit(self, X, y, support=None):\n",
    "        \"\"\"Fit the model according to the given training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Training data\n",
    "\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target values.\n",
    "\n",
    "        support : boolean np.array of shape (n_samples,), default = None\n",
    "            Array for support vector selection.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            An instance of the estimator.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = check_X_y(X, y, multi_output=True, dtype='float')\n",
    "\n",
    "        if not support:\n",
    "            self.support_ = np.ones(X.shape[0], dtype=bool)\n",
    "        else:\n",
    "            self.support_ = check_array(support, ensure_2d=False, dtype='bool')\n",
    "\n",
    "        self.support_vectors_ = X[self.support_, :]\n",
    "        support_labels = y[self.support_]\n",
    "\n",
    "        self.K_ = self.kernel_func(X, self.support_vectors_)\n",
    "        omega = self.K_.copy()\n",
    "        np.fill_diagonal(omega, omega.diagonal()+self.support_/self.C)\n",
    "\n",
    "        D = np.empty(np.array(omega.shape) + 1)\n",
    "\n",
    "        D[1:, 1:] = omega\n",
    "        D[0, 0] = 0\n",
    "        D[0, 1:] = 1\n",
    "        D[1:, 0] = 1\n",
    "\n",
    "        shape = np.array(support_labels.shape)\n",
    "        shape[0] += 1\n",
    "        t = np.empty(shape)\n",
    "\n",
    "        t[0] = 0\n",
    "        t[1:] = support_labels\n",
    "\n",
    "        # TODO: maybe give access to  lsmr atol and btol ?\n",
    "        try:\n",
    "            z = lsmr(D.T, t)[0]\n",
    "        except:\n",
    "            z = np.linalg.pinv(D).T @ t\n",
    "\n",
    "        self.bias_ = z[0]\n",
    "        self.alpha_ = z[1:]\n",
    "        self.alpha_ = self.alpha_[self.support_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'support_vectors_'):\n",
    "            raise NotFittedError\n",
    "\n",
    "        X = check_array(X, ensure_2d=False)\n",
    "        K = self.kernel_func(X, self.support_vectors_)\n",
    "        return (K @ self.alpha_) + self.bias_\n",
    "\n",
    "    def kernel_func(self, u, v):\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(u, v.T)\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            return rbf_kernel(u, v, gamma=self.gamma)\n",
    "\n",
    "        elif callable(self.kernel):\n",
    "            if hasattr(self.kernel, 'gamma'):\n",
    "                return self.kernel(u, v, gamma=self.gamma)\n",
    "            else:\n",
    "                return self.kernel(u, v)\n",
    "        else:\n",
    "            # default to linear\n",
    "            return np.dot(u, v.T)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        from scipy.stats import pearsonr\n",
    "        p, _ = pearsonr(y, self.predict(X))\n",
    "        return p ** 2\n",
    "\n",
    "    def norm_weights(self):\n",
    "        A = self.alpha_.reshape(-1, 1) @ self.alpha_.reshape(-1, 1).T\n",
    "\n",
    "        W = A @ self.K_[self.support_, :]\n",
    "        return np.sqrt(np.sum(np.diag(W)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20309c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2238e",
   "metadata": {},
   "source": [
    "# Stacked Ensemble Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45c9c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stack_ensemble_Prediction(Station_Name):\n",
    "    plt.figure(figsize=(13, 6))\n",
    "\n",
    "    \n",
    "    X, y = Station_Name['ZWD'].to_numpy().reshape(-1, 1), Station_Name['ZTD_igs'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[:-500], y[:-500], test_size=(len(y)-len(y[:-500])),random_state=0)\n",
    "\n",
    "    lssvr_model = LSSVR()\n",
    "    nn_model = MLPRegressor()\n",
    "    xgb_model = XGBRegressor()\n",
    "    \n",
    "    stacked_model = StackingCVRegressor(\n",
    "    regressors=[xgb_model, lssvr_model, nn_model],\n",
    "    meta_regressor=LinearRegression()) \n",
    "    \n",
    "    stacked_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = stacked_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse**0.5\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    models = ['xgb_model','LS-SVR', 'Neural Network', 'Stacked Ensemble']\n",
    "    color = ['red','blue','green','orange']\n",
    "    score = []\n",
    "\n",
    "    for model_score in [xgb_model,lssvr_model, nn_model, stacked_model]:\n",
    "        cv_scores = -cross_val_score(model_score, X_train, y_train, scoring='neg_root_mean_squared_error')\n",
    "        score.append(min(cv_scores))\n",
    "        \n",
    "    sns.barplot(models, score ,palette=color)\n",
    "    plt.title('Base Model Performance')\n",
    "    print(pd.DataFrame({'Date':Station_Name[-500:].index.values,'Actual':y_test.flatten(),'Predicted':y_pred}))\n",
    "    return y_pred\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e0493b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack_ensemble_Prediction(ADIS_STATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61e1ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_Model_Comparisoms():\n",
    "    fig, axs = plt.subplots(2, 2,figsize=(12, 8))\n",
    "    Models = ['Stacked_model','Autoregressive','Moving Average','ARIMA']\n",
    "    color = ['red','blue','green','orange']\n",
    "    ABPO_accuracy_Score =[0.004362919300503391,0.04863937221224741,0.04921956963363566,0.048943436374477275]\n",
    "    \n",
    "    ADIS_accuracy_Score = [0.002155871885795152,0.02779981926741842,0.027985644339273124,0.029457628455895148]\n",
    "    \n",
    "    MAL2_accuracy_Score=[0.005190404870723981,0.039897567581271665,0.040397887293639065,0.03997494748605169]\n",
    "    \n",
    "    NURK_accuracy_Score = [ 0.0003102568194383033,0.04961848133128715,0.051115772656624314,0.05462753868196547]\n",
    "    \n",
    "    axs[0, 0].bar(Models, ABPO_accuracy_Score,color = color)\n",
    "    axs[0, 0].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION ABPO')\n",
    "    \n",
    "    plt.setp(axs[0, 0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    axs[0, 1].bar(Models, ADIS_accuracy_Score,color = color)\n",
    "    axs[0, 1].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION ADIS')\n",
    "    plt.setp(axs[0, 1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    axs[1, 0].bar(Models, MAL2_accuracy_Score,color = color)\n",
    "    axs[1, 0].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION MAL2')\n",
    "    plt.setp(axs[1, 0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    axs[1, 1].bar(Models, NURK_accuracy_Score,color = color)\n",
    "    axs[1, 1].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION NURK')\n",
    "    plt.setp(axs[1, 1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    for index,value in enumerate(ABPO_accuracy_Score):\n",
    "        axs[0, 0].text(index, value, str(round(value,4)), ha='center', va='bottom',fontsize=12) if index == 0 else axs[0, 0].text(index, value-0.01, str(round(value,4)), ha='center', va='bottom',fontsize=12)\n",
    "    for index,value in enumerate(ADIS_accuracy_Score):\n",
    "        axs[0, 1].text(index, value, str(round(value,4)), ha='center', va='bottom',fontsize=12) if index==0 else axs[0, 1].text(index, value-0.006, str(round(value,4)), ha='center', va='bottom',fontsize=12)\n",
    "    for index,value in enumerate(MAL2_accuracy_Score):\n",
    "        axs[1, 0].text(index, value, str(round(value,4)), ha='center', va='bottom',fontsize=12) if index ==0 else axs[1, 0].text(index, value-0.007, str(round(value,4)), ha='center', va='bottom',fontsize=12)\n",
    "    for index,value in enumerate(NURK_accuracy_Score):\n",
    "        axs[1, 1].text(index, value, str(round(value,4)), ha='center', va='bottom' ,fontsize=12) if index == 0 else axs[1, 1].text(index, value-0.01, str(round(value,4)), ha='center', va='bottom' ,fontsize=12)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    fig.suptitle('MODEL ACCURACY ASSESSMENT BASED ON ROOT MEAN SQUARE ERROR')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a7721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d64fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correlate_Time_Series_with_Stacked_Ensemble(Station_Name):\n",
    "    station1_predicted_values = pd.Series(Stack_ensemble_Prediction(Station_Name))\n",
    "    station2_predicted_values = Autoregressive_Forecasting(Station_Name,1)[3]\n",
    "   \n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.plot(station1_predicted_values,station2_predicted_values)\n",
    "#     plt.plot(station2_predicted_values)\n",
    "    plt.show()\n",
    "    print(station1_predicted_values,station2_predicted_values)\n",
    "   \n",
    "    return station1_predicted_values.corr(station2_predicted_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e864320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Read_GNSS_Data(\"IGS_ALL_DATA_DAILY_OSAH_imputed.csv\")\n",
    "ABPO_STATION = Station_Extraction('ABPO',data)\n",
    "ADIS_STATION = Station_Extraction('ADIS',data)\n",
    "MAL2_STATION = Station_Extraction('MAL2',data)\n",
    "NURK_STATION = Station_Extraction('NURK',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4308fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate_Time_Series_with_Stacked_Ensemble(ADIS_STATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca792ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05b81f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_Model_Comparisoms_AIC():\n",
    "    fig, axs = plt.subplots(2, 2,figsize=(12, 8))\n",
    "    Models = ['Autoregressive','Moving Average','ARIMA']\n",
    "    color = ['blue','green','orange']\n",
    "    ABPO_accuracy_Score =[-12409.075,-10126.322,-12705.859]\n",
    "    \n",
    "    ADIS_accuracy_Score = [-14752.085,-12359.645,-14882.397]\n",
    "    \n",
    "    MAL2_accuracy_Score=[-12390.739,-10856.631,-12539.575]\n",
    "    \n",
    "    NURK_accuracy_Score = [ -13318.719,-9837.557,-13347.311]\n",
    "    \n",
    "    axs[0, 0].bar(Models, ABPO_accuracy_Score,color = color)\n",
    "    axs[0, 0].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION ABPO')\n",
    "    \n",
    "    plt.setp(axs[0, 0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    axs[0, 1].bar(Models, ADIS_accuracy_Score,color = color)\n",
    "    axs[0, 1].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION ADIS')\n",
    "    plt.setp(axs[0, 1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    axs[1, 0].bar(Models, MAL2_accuracy_Score,color = color)\n",
    "    axs[1, 0].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION MAL2')\n",
    "    plt.setp(axs[1, 0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    axs[1, 1].bar(Models, NURK_accuracy_Score,color = color)\n",
    "    axs[1, 1].set_title('PREDICTION ACCURACY ASSESSMENT AT STATION NURK')\n",
    "    plt.setp(axs[1, 1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    for index,value in enumerate(ABPO_accuracy_Score):\n",
    "        axs[0, 0].text(index, value, str(round(value,4)), ha='center', va='bottom',fontsize=12) if index == 0 else axs[0, 0].text(index, value-0.01, str(round(value,4)), ha='center', va='bottom',fontsize=12)\n",
    "    for index,value in enumerate(ADIS_accuracy_Score):\n",
    "        axs[0, 1].text(index, value, str(round(value,4)), ha='center', va='bottom',fontsize=12) if index==0 else axs[0, 1].text(index, value-0.006, str(round(value,4)), ha='center', va='bottom',fontsize=12)\n",
    "    for index,value in enumerate(MAL2_accuracy_Score):\n",
    "        axs[1, 0].text(index, value, str(round(value,4)), ha='center', va='bottom',fontsize=12) if index ==0 else axs[1, 0].text(index, value-0.007, str(round(value,4)), ha='center', va='bottom',fontsize=12)\n",
    "    for index,value in enumerate(NURK_accuracy_Score):\n",
    "        axs[1, 1].text(index, value, str(round(value,4)), ha='center', va='bottom' ,fontsize=12) if index == 0 else axs[1, 1].text(index, value-0.01, str(round(value,4)), ha='center', va='bottom' ,fontsize=12)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    fig.suptitle('MODEL ACCURACY ASSESSMENT BASED ON  AKAIKE INFORMATION CRITERIA(AIC)')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b5b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894b21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
